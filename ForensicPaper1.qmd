---
title: "Application of forensic meta-science to selected orthopaedic papers: Paper 1 10.1177/0363546506296042 (Pincewski et al 2007)"
author: "Corey Scholes"
affiliation: "EBM Analytics"
version: 1.0
date: "2025-Feb-15"
date-modified: "2025-Feb-15"
type: website
editor: visual
code-annotations: true
execute: 
  echo: false
  warning: false
  message: false
format:
  html:
    toc: true
    number-sections: true
    code-fold: true
    
bibliography: ForensicPapers.bib
---

# Analysis Preamble

This analysis acts as a training activity to apply techniques described in [Heathers 2024](https://jamesheathers.curve.space/).

I will extend this approach, by encapsulating a review within the contemporary reporting guidelines for the trial reported in the paper in question.

## My Conflicts Up Front

-   I've consulted for Smith and Nephew (see below) for a small engagement \>3 years ago.

-   I would call myself an acquaintance of the study authors

    -   I have co-authored a paper with Justin Roe [@Oussedik2012].

-   I have performed the same analyst role as the person responsible in this trial

-   Am I jealous of the citation record of this article - absolutely not, my interest here is for us to be able to move the conversation forward into the contemporary period and better serve future patients.

## Preparation

Load up required packages in advance. Citations applied to each library at first use in the text.

```{r}

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  "pwr",
  "confintr",
  "simstudy",
  "statcheck",
  "gert",
  "rcrossref",
  "httr",
  "jsonlite",
  "synthesisr",
  "reshape",
  "future",
  "furrr",
  "memoise",
  "googlesheets4",
  "openxlsx2",
  "readr",
  "purrr",
  "tidyverse",
  "tidymodels",
  "tidytext",
  "stopwords",
  "tictoc",
  "lubridate",
  "forcats",
  "gt",
  "consort",
  "gtsummary",
  "flextable",
  "survival",
  "ggplot2",
  "ggdist",
  "ggsurvfit",
  "ggfortify",
  "mice",
  "marginaleffects",
  "patchwork",
  "naniar",
  "quantreg",
  "broom",
  "broom.helpers",
  "labelled",
  "epoxy",
  "broom.mixed",
  "lme4",
  "janitor",
  "progressr",
  "DT",
  install = TRUE,
  update = FALSE
)

```

## Pull Retraction Watch

Cloned the RW git repo to the environment to bounce off later.

```{r}

# Set the repository URL and local path
#repo_url <- "https://gitlab.com/crossref/retraction-watch-data"
local_path <- "~/GitHub/retraction-watch-data"  # This will create a folder in your working directory

# Clone the repository
#git_clone(url = repo_url, path = local_path)
```

```{r}

# Update the repository (equivalent to git pull)
git_pull(repo = local_path)


```

Load in database

```{r}
Retractions <- read.csv(file.path(local_path, "retraction_watch.csv"))
```

# Selected Citation

[@pinczewski2007]

```{r}

targetDOI = "10.1177/0363546506296042"
```

```{epoxy}

Full citation - {rcrossref::cr_cn(dois = targetDOI, format = "text", style = "apa")}

```

## Selection Rationale

The selected paper appears at number 2 in "The top 100 most cited articles in Australian orthopaedic surgery" [@mcmillan2024].

It has been cited 288 times (pubmed) or 524 times (crossref), depending on which citation database you trust the most.

[![Metrics of selected citation as of 15-Feb-2025](AJSM%20Metrics.png)](https://journals.sagepub.com/doi/10.1177/0363546506296042)

# Bibliometric Analysis

## The paper

Check if it appears in the RW database

```{r}

TargetCitation <- rcrossref::cr_works(dois = "10.1177/0363546506296042")

RWPaper <- Retractions |> dplyr::filter(
  stringr::str_detect(OriginalPaperDOI,targetDOI) 
)

```

```{r}

if_else(
  length(RWPaper) < 1,
  "No Retraction Found",
  epoxy::epoxy("There were {nrow(RWPaper)} retractions identified")
)
```

There were 0 comments for this paper on PubPeer \[ 15-Feb-2025\].

## The authors

So I wrote a function to work through the surnames and bounce them against the RW database. We can argue about validation of this step until the cows come home, but for now I'm fairly satisfied that the individual authors do not have any retractions.

You can see for yourself in the matches table below (maybe I missed one?).

```{r}
library(tidyverse)
library(stringr)
library(stringdist)
library(memoise)
library(parallel)
library(furrr)
library(progressr)

# Set up parallel processing
setup_parallel <- function(cores = NULL) {
  if (is.null(cores)) {
    cores <- parallel::detectCores() - 1
  }
  plan(multisession, workers = cores)
}

# Create memoized helper functions
mem_stringdist <- memoise(stringdist)
mem_str_detect <- memoise(str_detect)

search_retractions_by_authors <- function(author_names, 
                                        retraction_data, 
                                        fuzzy_threshold = 0.2,
                                        date_from = NULL,
                                        date_to = NULL,
                                        cores = NULL) {
  
  # Initialize parallel processing
  setup_parallel(cores)
  
  # Progress function wrapped in with_progress
  with_progress({
    p <- progressor(steps = 4)
    
    # Convert author names to lowercase for case-insensitive matching
    author_names_lower <- tolower(author_names)
    p(amount = 1, message = "Initializing...")
    
    # Process retraction data
    matching_retractions <- retraction_data %>%
      # Convert dates to proper format
      mutate(
        RetractionDate = as.Date(RetractionDate, format = "%m/%d/%Y"),
        OriginalPaperDate = as.Date(OriginalPaperDate, format = "%m/%d/%Y")
      ) %>%
      # Apply date filters if provided
      {if (!is.null(date_from)) filter(., RetractionDate >= as.Date(date_from)) else .} %>%
      {if (!is.null(date_to)) filter(., RetractionDate <= as.Date(date_to)) else .} %>%
      # Convert Author column to lowercase for matching
      mutate(authors_lower = tolower(Author))
    
    p(amount = 1, message = "Processing data...")
    
    # Perform exact matching
    matching_retractions <- matching_retractions %>%
      mutate(
        exact_matches = future_map_chr(
          authors_lower,
          function(author_string) {
            matches <- author_names_lower[sapply(author_names_lower, function(name) {
              str_detect(author_string, str_c("\\b", name, "\\b"))
            })]
            str_c(matches, collapse = "; ")
          },
          .progress = TRUE
        )
      )
    
    p(amount = 1, message = "Performing exact matching...")
    
    # Perform fuzzy matching
    matching_retractions <- matching_retractions %>%
      mutate(
        fuzzy_matches = future_map_chr(
          authors_lower,
          function(author_string) {
            individual_authors <- str_split(author_string, ";")[[1]] %>%
              str_trim()
            
            matches <- author_names_lower[sapply(author_names_lower, function(search_name) {
              min_dist <- min(sapply(individual_authors, function(x) 
                stringdist(x, search_name, method = "jw")))
              min_dist <= fuzzy_threshold
            })]
            
            str_c(unique(matches), collapse = "; ")
          },
          .progress = TRUE
        )
      )
    
    p(amount = 1, message = "Performing fuzzy matching...")
    
    # Filter and select columns
    results <- matching_retractions %>%
      filter(exact_matches != "" | fuzzy_matches != "") %>%
      select(
        Title,
        Author,
        exact_matches,
        fuzzy_matches,
        Journal,
        Publisher,
        Country,
        RetractionDate,
        OriginalPaperDate,
        RetractionDOI,
        OriginalPaperDOI,
        Reason,
        everything(),
        -authors_lower
      )
    
    # Generate summary statistics
    summary_stats <- list(
      total_matches = nrow(results),
      matches_by_year = results %>%
        mutate(year = format(RetractionDate, "%Y")) %>%
        count(year, name = "retractions") %>%
        arrange(desc(year)),
      matches_by_publisher = results %>%
        count(Publisher, sort = TRUE, name = "retractions"),
      matches_by_country = results %>%
        count(Country, sort = TRUE, name = "retractions"),
      exact_vs_fuzzy = c(
        exact = sum(results$exact_matches != ""),
        fuzzy_only = sum(results$exact_matches == "" & results$fuzzy_matches != "")
      ),
      date_range = c(
        min = min(results$RetractionDate, na.rm = TRUE),
        max = max(results$RetractionDate, na.rm = TRUE)
      )
    )
    
    # Return results within with_progress
    list(
      matches = results,
      summary = summary_stats
    )
  })
}

# Clear memoization cache function
clear_retraction_search_cache <- function() {
  forget(mem_stringdist)
  forget(mem_str_detect)
  gc()  # Run garbage collection
}


```

```{r}
# Extract just family names
author_surnames <- TargetCitation$data$author[[1]] %>%
  pull(family)

# Or for full names (combining given and family)
author_full_names <- TargetCitation$data$author[[1]] %>%
  mutate(full_name = paste(given, family)) %>%
  pull(full_name)
```

```{r}

# For console progress bar
handlers("progress")

# Or for more detailed progress messages
handlers("txtprogressbar")

# Run the search with progress reporting
with_progress({
  results <- search_retractions_by_authors(
    author_surnames,
    Retractions,
    fuzzy_threshold = 0.2,
    date_from = "2020-01-01",
    date_to = "2024-12-31"
  )
})
```

```{r}


library(gt)

AuthorMatches <- results$matches

gt::gt(AuthorMatches |> dplyr::select(
  Title,
  Author,
  Journal,
  Country,
  exact_matches,
  fuzzy_matches,
  RetractionDate
)) |>
  tab_options(
    table.width = px(1200),  # Wider overall table
    table.font.size = px(12)  # Slightly smaller font
  ) |>
  cols_width(
    Title ~ px(200),          # Wider for full titles
    Author ~ px(200),         # Space for multiple authors
    Journal ~ px(200),        # Space for journal names
    Country ~ px(150),        # Space for multiple countries
    exact_matches ~ px(75),     # Wide space for institutions
    fuzzy_matches ~ px(75),     # Wide space for institutions
    RetractionDate ~ px(75)     # Wide space for institutions
  ) |>
  fmt_markdown(columns = everything()) |>
  text_transform(
    locations = cells_body(),
    fn = function(x) {
      vapply(x, function(x) {
        if (is.character(x)) {
          # Insert line breaks in long strings
          gsub("([,;]) ", "\\1\n", x)
        } else {
          as.character(x)
        }
      }, character(1))
    }
  ) |>
  tab_style(
    style = cell_text(whitespace = "pre-wrap"),  # Preserve line breaks
    locations = cells_body()
  ) |>
  cols_align(
    align = "left",
    columns = everything()
  ) |>
  knitr::knit_print()

```

# Reporting Framework

To provide some structure to scrutinising this article - the following guidelines were utilised.

CONSORT 2010 - [@moher2010]

CONSORT Abstracts - [@hopewell2008]

CONSORT Parallel group trials - [@moher2010; @consort2010]

CONSORT Nonpharmacologic treatments - [@boutron2017]

CONSORT PRO - [@calvert2013]

CONSORT Harms - [@junqueira2023]

# Trial Information

So first things first, this study is not a randomised trial. In the methods, it's described as a "prospective controlled trial". More on that later.

## CONSORT \[23\] Registration

No prior trial registration in clinicaltrials.gov. Manual search 17-Feb-2025.

## CONSORT \[24\] Trial Protocol

This gets a little bit more interesting. The protocol is not described fully in this paper, but is somewhat described throughout the multiple previous papers from the same trial. As far as I can tell though, there was no protocol published prior to the first follow up paper.

Study with follow up at 7 years - [@roe2005]

Study with follow up at 5 years - [@pinczewski2002]

The role of a data monitoring committee.

## CONSORT \[25\] Funding

Funding provided by

-   The Australian Institute of Musculoskeletal Research

    -   A local not-for-profit entity to support research within a group of surgeons in which the senior author was a key influencer

-   Smith and Nephew Endoscopy

    -   Manufacturer of the fixation and surgical instrumentation for the ACL reconstruction procedures

    -   Fund fellowships for less experienced surgeons to train under the senior author

# Title and Abstract

## CONSORT \[1a\] Trial Identification

The title identifies the study as a "prospective controlled trial".

## CONSORT \[1b\] Structured summary

<!--# Structured summary of trial design, methods, results, and conclusions -->

<!--# P1b: The PRO should be identified in the abstract as a primary or secondary outcome -->

The PROs used in the study are *not* identified in the abstract as primary or secondary outcomes in the trial.

# Introduction

## CONSORT \[2a\] Scientific Background and Explanation

<!--# Including background and rationale for PRO assessment -->

## CONSORT \[2b\] Specific Objectives or Hypotheses

<!--# Specific objectives or hypotheses for outcomes of benefits and harms -->

<!--# P2b: The PRO hypothesis should be stated and relevant domains identified, if applicable -->

# Methods - External assessment

This paper should have been put through the wringer by SR-MA of graft type in ACL reconstruction. Let's do a quick summary of what others have said about the methods through RoB or methodology assessments in previous reviews.

I retrieved the citation list for this paper from *citationchaser* (the shiny app, I cant be bothered signing up for a plan for the API access) [@Haddaway2022]. It conducts a search through lens.org.

```{r}

Citations <- synthesisr::read_refs(
  filename = "C:/Users/cscho//Documents/GitHub/Forensic-Paper1/Paper1Citations",
  tag_naming = "wos",
  return_df = TRUE,
  verbose = TRUE
)

```

Filter down for recent systematic reviews

```{r}

CitationsSR <- Citations |> dplyr::filter(
  stringr::str_detect(title,"(systematic.*review)|(meta.*analysis)")
) |> dplyr::arrange(
  year
)

```

## Risk of Bias

One systematic review [@janssen2017] used the Cochrane library checklist for RCTs, with two reviewers conducting independent ratings. The authors classified this trial as a *clinical controlled trial* (no formal randomisation). The overall rating from the combined checklist score was *Questionable (*score was between 30% and 50%).

A more recent review [@sollberger2022] attempted to assess long term results after ACLR with PT vs HS autograft with a min follow up of 10 years, but did not include this paper in their analysis.

Another review [@migliorini2020] included it in their quality assessment, but did not break down their RoB ratings by individual paper.

# Methods - Trial Design

## CONSORT \[3a\] Description of trial design

<!--# Include allocation ratio -->

Prospective controlled trial.

This doesnt really have any meaning. Is it that the HS group is being compared to the PT control? Let's assume that's the case. This is really an interrupted cohort study, or an interrupted time series.

<!--# Come back to this later -->

## CONSORT \[3b\] Changes to methods after trial commencement

<!--# Come back and check this -->

Changes to eligibility may have altered or been reinterpreted at some point after trial commencement.

# Methods - Participants

## CONSORT \[4a\] Eligibility criteria

Recreate eligibility for clarity

| Category | Inclusion | Exclusion | Comments |
|------------------|------------------|------------------|------------------|
| Patient | Desire return to competitive activity |  |  |
| Pathology | Complete ACL rupture | Additional ligament injury such that reconstruction was deemed necessary |  |
|  | Grade 2 Lachman (or above) | Chondral damage |  |
|  | Positive pivot shift |  |  |
| Treatment |  | Meniscal damage such that meniscectomy of \>1/3 of one (medial or lateral) meniscus was required |  |
| Other | Failed nonoperative treatment | Previous meniscectomy |  |
|  |  | Abnormal radiographic image |  |
|  |  | Abnormal contralateral knee |  |
|  |  | Patients seeking compensation for their injury |  |
|  |  | Patients who did not wish to participate in a research program |  |

<!--# The representativeness of the sample is questionable -->

Yes, the representativeness of the sample is questionable for reasons.

<!--# Does a trial need to be representative? -->

A trial of this type need not be representative, as the aims of a trial are to "Provide evidence for **relative** treatment effectiveness over an adequate time horizon for assessing target patient outcomes" ([Harrell, 2023](https://www.fharrell.com/post/rct-mimic/ "RCT generalisability")). Harrell goes on to note that knowledge of the relative effect from an RCT combined with the absolute risk of an outcome from observational data can be combined to assess individual treatment efficacy. Others have observed that treatment effects can remain the same even when trial participants differ from the target population [@bradburn2020], this needs to be replicated in our present context here though. Still others have argued against the ingrained notion in clinical science that representative patient samples are the only mechanism by which findings can be appropriately generalised [@rothman2013].

The authors *have* mentioned that the trial can be used to establish a baseline of outcomes in this population, and I think this an overreach given the limitations on its external validity.

<!--# When applicable, eligibility criteria for centers and for care providers -->

Single-centre and single-surgeon study, eligibility criteria for centres and care providers not required.

## CONSORT \[4b\] Settings and locations of data collection

<!--# why is this important - retrieve from guidance and elaboration. "Information...is crucial to judge the applicability and generalisability of the trial. Other aspects...may also affect a study's external validity. -->

# Methods - Interventions

## CONSORT \[5\] Interventions for each Group

# Methods - Outcomes

## CONSORT \[6a\] Primary and Secondary

<!--# Completely defined pre-specified primary and secondary outcome measures, including how and when they were assessed -->

| Category | Outcome | Type | Comments |
|------------------|------------------|------------------|------------------|
| Adverse Events | **Ipsilateral graft intact** | Time to event |  |
|  | Contralateral ligament intact | Binary; Time to event |  |
|  | Ipsilateral adverse events (complications) | Binary |  |
|  | Ipsilateral further surgery | Binary |  |
|  | Harvest site symptoms | Ordinal |  |
| Subjective symptoms | **Symptoms with strenuous activity (IKDC)** | Ordinal |  |
|  | Lysholm Knee Score | Continuous |  |
|  | Sports participation | Binary |  |
|  | Knee-related reduction in activity | Binary |  |
|  | Kneeling pain | Continuous |  |
| Objective testing | **Lachman Grade** | Ordinal |  |
|  | Pivot Shift | Binary |  |
|  | Instrumented Laxity | Binary |  |

## CONSORT \[6b\] Changes after commencement

Outcome switching from trial plan - commencement to analysis and reporting is problematic in trials [@altman2017]. This is one of the key weaknesses of this work - the outcomes have been potentially switched from one paper to the next based on interest over time. With no firm primary outcome stated early on in the reporting process or pre-specified, it is entirely possible that reported outcomes have been prioritised based on i) altered levels of interest over time or ii) levels of significance at the time of comparison.

# Methods - Sample size

This is the biggest mystery of this paper I think. The methods description is very clear about what the sample size is, but really nothing about how it was determined. The only mention of power is in the discussion of [@pinczewski2002] to say that the study is underpowered for detecting a difference in graft rupture.

Ultimately, I can run some simulations here to prove the point, but I just dont think this study was adequately powered for any of the comparisons it has made between the groups.

## CONSORT \[7a\] Determination

Not described in the paper.

```{r}
pwr.p.test(h = ES.h(p1 = 0.75, p2 = 0.50),
           sig.level = 0.05,
           power = 0.80,
           alternative = "greater")
```

## CONSORT \[7b\] Interim Analyses and Stopping

The series of papers making up reporting of this trial really needed a more comprehensive approach to stopping parameters and planning for interim analyses, at what turned out to be 5, 7 and 10 year follow ups.

Interim analyses for efficacy allow a trial to be stopped early [@ciolino2023]. With the benefit of hindsight, there are a number of issues with long-term follow up of ACL reconstruction that confound a comparison between graft types at such a long follow up period. Namely, the incidence and severity of osteoarthritis present in each group. Considering the quality of reconstruction is one (arguably minor) component of osteoarthritis after ACL rupture, the validity of the comparison could be questioned.

In defence of the authors, the natural history of post-ACL rupture OA was still a developing concept when this study was implemented. Nevertheless, the lack of planning of longer-term followup or interim analyses and how they are compensated in the comparisons needs to be addressed in more detail.

# Methods - Randomisation

This was an attempted RCT that failed due to recruitment refusal.\
\
"*On October 28, 1993, we began a prospective randomized study of consenting patients who met the required criteria. However, by April 10, 1994, although 52 patients had been randomized, no further patients agreed to participate in randomization. This was because the patients were noticing, through comments from the physical therapist, that the use of hamstring tendon graft led to a more rapid recovery from surgery.*" [@pinczewski2002]

I dont even know what to do with this.

Clinical equipoise (didnt exist it seems)

Patient selection - exclusivity (patients were self-selected based on perioperative care and functional constraints imposed)

<!--# Why the results cannot be intrepreted the same way as an RCT -->

<!--# What could have been done to strengthen the comparisons  -->

## CONSORT \[8a\] Sequence Method

## CONSORT \[8b\] Sequence Type

<!--# Type of randomisation; details of any restriction (such as blocking and block size) -->

## CONSORT \[9\] Allocation concealment

<!--# Mechanism used to implement the random allocation sequence (such as sequentially numbered containers),describing any steps taken to conceal the sequence until interventions were assigned -->

## CONSORT \[10\] Implementation

<!--# Who generated the random allocation sequence, who enrolled participants, and who assigned participants to interventions -->

## CONSORT \[11a\] Blinding (Who)

<!--# If done, who was blinded after assignment to interventions (for example, participants, care providers, those assessing outcomes) and how -->

There is no mention of blinding in any of the articles related to this trial. Even if randomisation didnt occur, blinding of everyone involved in the trial (as much as practicable), but particularly those responsible for measurement and analysis is crucial to mitigate potential biases. Especially when industry funding is in the mix.

## CONSORT \[11b\] Intervention Similarity

Not applicable

# Methods - Statistical Methods

The analysis description in this paper, definitively comparing graft types in ACLR reconstruction, is a total of four sentences. I just don't think this would (or should) pass muster in the contemporary period.

The data structure clearly has dependency within patients over time points, but is likely under-sampled to cater for a fully specified model (mixed effects). See *Sample Size.*

Let's breakdown each sentence;

*The outcomes were compared between the two groups at 10 years using the Mann-Whitney U test for the continuous measurements (KT1000, range of motion, Lysholm score) and ordered categorical variables (such as IKDC categories).*

Well, this is a cracking start - theoretically an MW U test can be applied to ordinal categorical responses, but gee-whiz, there are better approaches to use, particularly in this context. First of all, how were ties between groups corrected for? I suspect they werent and that means for a study that was probably already on the limits of power for the implied comparisons, using this test in this way chips away at what little power remained. Also, many of the variables are binary responses, which this test really wouldnt be appropriate for.

Second, "at 10 years" is a bit misleading, as comparisons at the earlier time points are also made.

*Wilcoxon signed rank test was used to assess changes* *over time.*

Combining t-tests like this for within and between-group comparisons is quite the sign that a more comprehensive model should be employed here.

*Linear regression analysis was used to assess relationship between selected dependent and independent variables.*

So close, yet so far - linear regression gets a guernsey, but not to do with anything related to the aims of the paper. Almost made it.

*Statistical significance was assessed at the 5% level.*

While this is good to know, there are lots of other details we need before we get to this.

## CONSORT \[12a\] Comparison Between Groups

<!--# P12a: Statistical approaches for dealing with missing data are explicitly stated -->

There are a metric tonne of comparisons between groups in this paper. Let's see if we can table up what we're looking at.

Data assumption was non-parametric. <!--# Come back to this -->

| Category | Outcome | Type | Model - Test | Comments |
|---------------|---------------|---------------|---------------|---------------|
| Adverse Events | **Ipsilateral graft intact** | Time to event | Nil (KM used to establish survival rates) | A risk table would have been helpful |
|  | Contralateral ligament intact | Binary; Time to event | Nil (KM used to establish survival rates) | A risk table would have been helpful |
|  | Ipsilateral adverse events (complications) | Binary | Between-Group: Mann-Whitney U Test | Suspect not a valid test |
|  | Ipsilateral further surgery | Binary | Between-Group: Mann-Whitney U Test | Suspect not a valid test |
|  | Harvest site symptoms | Ordinal | Between-Group: Mann-Whitney U Test |  |
| Subjective symptoms | **Symptoms with strenuous activity (IKDC)** | Ordinal | Between-Group: Mann-Whitney U Test |  |
|  | Lysholm Knee Score | Continuous | Between-Group: Mann-Whitney U Test |  |
|  | Sports participation | Binary | Between-Group: Mann-Whitney U Test | Suspect not a valid test |
|  | Knee-related reduction in activity | Binary | Between-Group: Mann-Whitney U Test | Suspect not a valid test |
|  | Kneeling pain | Continuous | Between-Group: Mann-Whitney U Test |  |
| Objective testing | **Lachman Grade** | Ordinal | Between-Group: Mann-Whitney U Test |  |
|  | Pivot Shift | Binary | Between-Group: Mann-Whitney U Test | Suspect not a valid test |
|  | Instrumented Laxity | Binary | Between-Group: Mann-Whitney U Test | Suspect not a valid test |
|  | Range of motion - extension | Binary | Between-Group: Mann-Whitney U Test | Suspect not a valid test |
|  | Single-leg hop test | Binary | Nil |  |
| Operative Findings | Meniscal Injury | Binary | Between-Group: Mann-Whitney U Test | Suspect not a valid test |
|  | Meniscal Treatment | Binary | Between-Group: Mann-Whitney U Test | Suspect not a valid test |
| Radiographic | IKDC grading x-ray | Ordinal | Between-Group: Mann-Whitney U Test |  |
| Composite | IDEAL outcome | Binary | Between-Group: Mann-Whitney U Test | Suspect not a valid test |

## CONSORT \[12b\] Additional Analyses

<!--# Methods for additional analyses, such as subgroup analyses and adjusted analyses -->

# Results

Let's roll through the results by section and make some notes as we go.

## CONSORT \[13a\] Participant Flow

<!--# For each group, the numbers of participants who were randomly assigned, received intended treatment, and were analysed for the primary outcome -->

Some general comments are;

-   The participant flow is explained in text, but this would be so much easier to understand with a flow chart. Especially trying to understand who was eligible for which outcomes at which follow up. I assume they didnt ask graft failure patients to be included at the post-failure timepoints for example.

-   There is clearly a concerted effort to retrieve patients (in some form) at 5 years, compared to the 4 year followup (Table 1 [@pinczewski2002]).

-   There is a risk of survivor bias in the PROMs assessment at 10 years with \~25% of cases failed by then. What would happen if a comparison were made based on ITT?

<!--# The number of PRO outcome data at baseline and at subsequent time points should be made transparent -->

The followup of patients at the final timepoint is noted in the follow-up section. This could be communicated more effectively with a comprehensive flowchart incorporating all published timepoints.

## CONSORT \[13b\] Losses and Exclusions per Group

<!--# For each group, losses and exclusions after randomisation, together with reasons -->

## CONSORT \[14a\] Recruitment Period

<!--# Dates defining the periods of recruitment and follow-up -->

PT Group - Jan-1993 to Apr-1994

HT Group - Oct-93 to Nov-1994

The authors state "senior author...after Apr-1994..used the HT graft exclusively". Does this comparison not include the learning curve for HT? Why not start recruitment of HT group after Apr-1994?

The specific followup dates are not included in any of the trial papers - this is particularly important for interpreting the PROMs.

## CONSORT \[14b\] Trial Termination

<!--# Why the trial ended or was stopped -->

No description of trial termination in the methods. However, there is a section in [@pinczewski2002] talking about abandoning the trial due to recruitment failure (see *Randomisation*).

## CONSORT \[15\] Baseline Data

<!--# A table showing baseline demographic and clinical characteristics for each group -->

<!--# Including baseline PRO data when collected -->

```{r}
Group1 = list(
  FemaleCount = 42,
  AgeMedian = 25,
  AgeMin = 13,
  AgeMax = 42
)
Group1$AgeSDEst = (Group1$AgeMax-Group1$AgeMin)/2

Group2 = list(
  FemaleCount = 43,
  AgeMedian = 24,
  AgeMin = 13,
  AgeMax = 52
)
Group2$AgeSDEst = (Group2$AgeMax-Group2$AgeMin)/2


```

Well, first of all - I'm not sure this would pass HREC in this day and age. ACL reconstruction in skeletally immature individuals is a different set of ethical considerations to adults and I'm not sure that would pass muster in the present environment. Considering the narrowness of the inclusion critera, adding in immature patients into the sample is an interesting choice.

The IKDC is also not validated for paediatric patients. In their defence, the paediatric version of the IKDC would not be published for another decade [@nasreddine2016]. It is also contended that the Lysholm is not validated for paediatric populations [@fabricant2020].

```{r}
SexCheck <- list(
  Group1 = prop.test(Group1$FemaleCount, 90, 0.95),
  NZRegistry = prop.test(8816,8816+11581,conf.level = 0.95)
)
```

```{epoxy}

The proportion of females {round(SexCheck$Group1$estimate)} ({round(SexCheck$Group$conf.int[1],digits = 2)}, {round(SexCheck$Group1$conf.int[2],digits = 2)}) is pretty much in line with the NZ ACL Registry {round(SexCheck$NZRegistry$estimate,digits = 3)} ({round(SexCheck$NZRegistry$conf.int[1],digits = 2)}, {round(SexCheck$NZRegistry$conf.int[2],digits = 2)})

```

Comparison between groups for age as reported

```{r}

SexCheck$GroupCom = prop.test(
  x = c(42,43),
  n = c(90,90),
  conf.level = 0.95
)

```

Now let's have a look at age.

```{r}
scrutiny::grim(as.character(Group1$AgeMedian), n = 90)
```

```{r}
scrutiny::grim(as.character(Group2$AgeMedian), n = 90)
```

Do a comparison - we have to assume the median and the means are the same in this sample.

```{r}

AgeCheck <- list(
  t_stat = (Group1$AgeMedian - Group2$AgeMedian) / sqrt((Group1$AgeSDEst^2 / 90) + (Group2$AgeSDEst^2 / 90)),
  df = (90 + 90) - 2
)

AgeCheck$p_value <- 2 * pt(-abs(AgeCheck$t_stat), AgeCheck$df)

```

```{epoxy}

Using a st

```

\

## CONSORT \[16\] Numbers Analysed

<!--# For each group, number of participants (denominator) included in each analysis and whether the analysis was by original assigned groups -->

## CONSORT \[17a\] Outcomes and Estimation

<!--# For each primary and secondary outcome, results for each group, and the estimated effect size and its precision (such as 95% confidence interval) -->

<!--# For multidimensional PRO results from each domain and time point -->

## Scrutiny

Let's try out some checks -see how far we get. Let's use *statcheck* first, see if we can get a bite.

```{r}
statcheck::checkPDF("~/Pinczewski et al 2007.pdf")
```

Not super great, but not surprising either - the results are not formatted in APA style (not unusual for the field).

Next, let's see if we can replicate a selection of the p-values. Let's simulate the groups.

```{r, warning= FALSE}
# Function to generate one dataset and run test
simulate_test <- function(seed) {
  set.seed(seed)
  
  # Generate data
  study_data <- simstudy::genData(149) |>
    simstudy::trtAssign(
      nTrt = 2,
      balanced = FALSE,
      ratio = 74:75,
      grpName = "GraftGrp"
    )
  
  # Define and add outcome
  study_def <- simstudy::defDataAdd(
    varname = "IdealOutcome",
    formula = "ifelse(GraftGrp == 1, 0.47, 0.69)",
    dist = "binary"
  )
  
  final_data <- simstudy::addColumns(study_def, study_data) |>
    dplyr::mutate(
      GraftGrp = forcats::as_factor(GraftGrp),
      IdealOutcome = as.numeric(IdealOutcome)
    )
  
  # Run test and extract statistics
  test_result <- stats::wilcox.test(
    IdealOutcome ~ GraftGrp,
    data = final_data,
    correct = TRUE,
    exact = TRUE
  )
  
  # Calculate effect size (difference in proportions)
  group_props <- tapply(final_data$IdealOutcome, final_data$GraftGrp, mean)
  effect_size <- diff(group_props)
  
  return(list(
    p_value = test_result$p.value,
    effect_size = effect_size
  ))
}

# Run bootstrap simulation
set.seed(2065) # For reproducibility of the simulation
n_iterations <- 1000
results <- data.frame(
  p_value = numeric(n_iterations),
  effect_size = numeric(n_iterations)
)

for(i in 1:n_iterations) {
  sim_result <- simulate_test(seed = i)
  results$p_value[i] <- sim_result$p_value
  results$effect_size[i] <- sim_result$effect_size
}

# Calculate summary statistics
summary_stats <- list(
  mean_pvalue = mean(results$p_value),
  p_value_95ci = confintr::ci_mean(results$p_value),
  mean_effect = mean(results$effect_size),
  effect_95ci = confintr::ci_mean(results$effect_size),
  sig_results = (nrow(results |> dplyr::filter(p_value < 0.05))/1000) * 100
)

# Print results
print("Bootstrap Simulation Results:")
print(paste("Mean p-value:", round(summary_stats$mean_pvalue, 4)))
print(paste("95% CI for p-value:", 
            round(summary_stats$p_value_95ci$interval, 4)))
print(paste("Mean effect size (difference in proportions):", round(summary_stats$mean_effect, 4)))
print(paste("95% CI for effect size:",
            round(summary_stats$effect_95ci$interval, 4)))
print(paste("Percentage of significant results:", 
            round(summary_stats$sig_results, 1), "%"))

# Create histogram of effect sizes
if (requireNamespace("ggplot2", quietly = TRUE)) {
  ggplot2::ggplot(results, ggplot2::aes(x = effect_size)) +
    ggplot2::geom_histogram(bins = 30, fill = "skyblue", color = "black") +
    ggplot2::theme_minimal() +
    ggplot2::labs(
      title = "Distribution of Effect Sizes Across Simulations",
      x = "Effect Size (Difference in Proportions)",
      y = "Count"
    )
}
```

```{r}
p.out <- pwr.2p.test(h = ES.h(p1 = 0.47, p2 = 0.69),
                    sig.level = 0.05,
                    power = 0.80,
                    alternative = "two.sided")
plot(p.out)
```

CONSORT \[17a2\] Outcomes Omitted

<!--# For outcomes omitted from the trial report (benefits and harms), provide rationale for not reporting and indicate where the data on omitted outcomes can be accessed -->

## CONSORT \[17b\] Binary Outcomes

<!--# For binary outcomes, presentation of both absolute and relative effect sizes is recommended -->

## CONSORT \[18\] Ancillary Analyses

<!--# Results of any other analyses performed, including subgroup analyses and adjusted analyses, distinguishing pre-specified from exploratory -->

## CONSORT \[19\] Harms

<!--# All important harms or unintended effects in each group (for specific guidance see CONSORT for harms) -->

# Discussion

## CONSORT \[20\] Limitations

<!--# Trial limitations, addressing sources of potential bias, imprecision, and, if relevant, multiplicity of analyses -->

<!--# P20/21: PRO–specific limitations and implications for generalizability and clinical practice -->

## CONSORT \[21\] Generalisability

<!--# Generalizability (external validity, applicability) of the trial findings -->

## CONSORT \[22\] Interpretation

<!--# Interpretation consistent with results, balancing benefits and harms, and considering other relevant evidence -->

<!--# PRO data should be interpreted in relation to clinical outcomes including survival data, where relevant -->

# References
